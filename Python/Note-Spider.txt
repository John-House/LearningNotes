'''
（非）结构性数据的提取
==========================================
1.1 re正则表达式(import re)
	-----------------------------------
	核心内容
		.      非\n的任意字符
		\w     任意的字母、数字、下划线和汉字   \W   任意的非字母、数字和下划线
		\d     任意的数字                   \D   任意的非数字
		\s     任意的空白字符                \S   任意的非空白字符
		\b     匹配单词边界                  \B   匹配非单词边界
		^      匹配字符串开头                 $   匹配字符串末尾
		[...]  匹配[]中的任意一个           [^...] 不匹配[]中的任意一个
		  |--> [a-z]
		  |--> [A-Z]
		  |--> [a-zA-Z]
	-----------------------------------
	数量修饰
		*      0个或多个【核心内容】         *?      非贪婪，尽可能匹配的少
		+      1个或多个【核心内容】	     +?      非贪婪，尽可能匹配的少
		？      0个或1个【核心内容】         ??      非贪婪，尽可能匹配的少
		{n}    n个【核心内容】
		{n,}   n个或多个【核心内容】         {n,}?   非贪婪，尽可能匹配的少
		{n,m}  最少n个，最多m个【核心内容】   {n,m}?  非贪婪，尽可能匹配的少

	断言修饰
		(?<=表达式)    表达式不能存在【数量修饰】，即匹配:【前面是“表达式”】的核心内容
		(?<!表达式)    表达式不能存在【数量修饰】，即匹配:【前面不是“表达式”】的核心内容

		(?=表达式)     表达式不能存在【数量修饰】，即匹配:【后面是“表达式”】的核心内容
		(?!表达式)     表达式不能存在【数量修饰】，即匹配:【后面不是“表达式”】的核心内容
	-----------------------------------
	分组统计
		(核心内容)           			捕获匹配的核心内容，并分组（组号为 1,2,3,...)），后面要引用形如：\1,\2,\3,...
		(?P<name>核心内容)   			捕获匹配的核心内容，并分组（组号为 name1,name2,name3,...），后面要引用（但不会被捕获）形如：(?P=name1),(?P=name2),(?P=name3)...

		(?:核心内容)         			不捕获匹配的核心内容
		(?#注释内容)		  			编写注释内容，注释内容不会被捕获

		(?(组号/name)模式1|模式2)		如果'组号/name'所对应的内容存在，则匹配模式1；否则匹配模式2
									注意：'组号/name'对应的分组内容后面应该加上'?'的数量修饰，即(.../?P<name>...)?
	-----------------------------------
	多种条件
		表达式 | 表达式	即匹配多种规则，但类似于【短路或】，条件书写的先后顺序很重要


1.2 BeautifulSoup库(from bs4 import BeautifulSoup)
	-----------------------------------
	# 创建BeautifulSoup对象（即HTML DOM模型）

	bsoup = BeautifulSoup(文件对象/Unicode类型的字符串对象，'解析器')

		--> 返回BeautifulSoup对象，即对应网页的document对象;'解析器'推荐使用lxml

		    在BeautifulSoup中，只有4种类型的对象：
	    	  		    Tag --> 标签对象
	    	  BeautifulSoup --> 文档对象（可以理解为标签对象）
			NavigableString --> 字符串对象（位于Tag对象中）
			        Comment --> 注释对象（特殊的NavigableString对象）

			标签对象的基本属性: .name      --> 返回字符串，包含标签名
						    .attrs     --> 返回字典，包含标签的所有属性
						    Tag['属性'] --> 返回对应Tag对应属性的值（如果该属性被定义成多值属性，则返回所有字段构成的列表）

	# 遍历BeautifulSoup对象，获取想要的内容

	bsoup.标签名
		--> 返回标签对象，即DOM模型中'第一个'对应'标签'的内容；bsoup对象也可以视作一个标签对象使用

		标签对象.contents
			--> 返回列表对象，由标签对象和空白字符（换行符）组成，包含该'标签'的所有'子节点'内容

		标签对象.children
			--> 返回迭代器对象，包含该'标签'的所有'子节点'内容

		标签对象.descendants
			--> 返回生成器对象，包含该'标签'下的所有'子孙节点'内容

		标签对象.string
			--> 返回字符串对象，包含该'标签'中的字符串内容（如果该'标签'下有多个子节点，则返回None）

		标签对象.strings
			--> 返回迭代器对象，包含该'标签'中多个子节点中的字符串内容

		标签对象.stripped_strings
			--> 返回迭代器对象，只是多个子节点中的字符串内容去掉了空白字符

		标签对象.parent
			--> 返回标签对象，包含该标签的'父节点'内容

		标签对象.parents
			--> 返回生成器对象，包含该标签(标签或字符串)的所有'父辈节点'内容

		标签对象.find_all(...)
			--> 返回列表对象，由标签对象下满足条件的所有'子孙节点'对应的标签对象组成

			    name='xx'，即标签名为xx的

			    属性=过滤器，即属性为xx的 --> 某些特殊属性需要写成atrrs={'属性名':'属性值'}的形式，
			    			  		      如果是class属性，由于class是关键字，所以写成class_
			    			  		      在bs4中，标签对象的class属性属于多值属性，可以使用属性值的部分字段

			    string=过滤器，即寻找指定的字符串，一般用正则来匹配内容

			    limit=数字，即只寻找指定数量的内容

			    recursive=True/False，即默认为True，在所有'子孙节点'中寻找；若改为False,则在所有'直接子节点'中寻找

				过滤器，可以为：字符串、正则对象、列表（内嵌字符串/正则对象）、True（相当于'所有'的意思）以及
							 函数名（仅支持1个参数，返回值为True则找到内容，False则未找到内容，如下）
							 --> def has_class_but_no_id(tag):
									return tag.has_attr('class') and not tag.has_attr('id')

			--> find_all()中多个参数可以协同过滤，即'与'的作用

		标签对象.find(...)，
			--> 等价于limit=1的find_all()方法

		标签对象.select()
			--> 通过CSS选择器来寻找满足条件的内容

		标签对象.select_one()，即顾名思义寻找第一个满足条件的内容

		标签对象.prettify(['编码代号'])
			--> 返回字符串对象，即格式化输出bsoup对象/标签内容，BeautifulSoup的默认输出编码均为'UTF-8'
			    			 如果想指定其他编码的输入请附上参数，或者bsoup/标签名.encode('编码代号')

		标签对象.get_text(['sep'],strip=True/False)
			--> 返回字符串对象，bsoup对象/标签下的所有子孙节点的字符串内容，
			   				 'sep'指定返回的内容中两两之间的'分隔符'是什么，
			   				 strip=True省略空白的字符串内容

1.3 lxml类库
	-----------------------------------


'''
 

'''
标准库的使用
==========================================
爬取http/https静态页面，使用到：
	· urllib.request
	· ssl
	· urllib.parse

	· 爬取Ajax页面，后台抓包获取json的data数据
	· 使用cookie爬取页面，创建CookieJar对象，并封装成HTTPCookieProcessor()对象，
					   然后封装成opener对象，然后然后open()方法发送request
	· 使用代理IP爬取页面，创建ProxyHandler对象，并封装成opener对象，
					  然后open()方法发送request

# 1.1 构造request请求

	full_url = '...'
		--> url中 ？和 & 作为分隔符，遇到中文且形如【xx=...】的part_url时需要用url编码进行转码，
			即import urllib.parse as up,
			part_url = up.urlencode({'xx':'...'}) --> urlencode()方法只接受字典类型的参数

	data = up.urlencode({...})
		--> 也需要留意进行url编码转码，urlencode()方法返回的是'百分号编码'后的字符串对象

	headers = {...}
		--> 主要是'User-Agent'的设置

	request = ur.Request(full_url,method='xx必须大写',data=data,headers=headers)

	# request对象具有以下常用的方法和属性：
		.full_url                      --> 请求的url
		.type                          --> 请求的protocol类型

		.header_items()                --> 请求头信息（以二元元组嵌套在列表中返回）
		.data                          --> 请求体信息，没有的话为None

		.set_proxy('host','protocol')  --> 为请求设置代理


# 1.2 发送request请求，拿到response响应

	# 1.2.1 通过urllib.request库封装好的urlopen()方法发送请求

	    * urlopen(url/request对象,data=None,context=None)
	      通常使用request对象作为urlopen()的参数，原因是：urlopen()方法本身无法修改'User-Agent'属性，
	      这样就很容易被服务器发现自己是爬虫程序

	   ## 如果出现SSL证书要求，
	   		import ssl
			context = ssl._create_unverified_context() --> 表示忽略证书认证的要求
			或者
			context = ssl.SSLContext()
			context.verify_mode = ssl.CERT_NONE        -->（推荐这种）也是忽略证书

			response = ur.urlopen(request,context=context)

	   ## 如果不出现SSL证书要求
			response = ur.urlopen(request)

	# 1.2.2 通过自定义handler对象，并封装成opener对象，然后调用opener.open()方法发送请求

	    # 如果是http页面
			http_handler = ur.HTTPHandler()
				--> 参数debuglevel=1 ，表示启动debug log功能，用于记录收发包的信息

	      如果是https页面
	   		import ssl
			context = ssl._creat_unverified_context()  --> 表示忽略证书认证的要求
			或者
			context = ssl.SSLContext()
			context.verify_mode = ssl.CERT_NONE        -->（推荐这种）也是忽略证书

			https_handler = ur.HTTPSHandler(debuglevel=0,context=context)

	   ## 封装成opener对象
	     	opener = ur.build_opener(http_handler/https_handler)

	  ### 调用open()方法发送请求
		    response = opener.open(request)

# 1.3 读取类文件中的内容
	data = response.read() --> data是bytes类型

	# 如果要求按照特定编码，转码成string类型
	str_html = data.decode('编码')
	或者
	str_html = str(data,'编码')

	# 如果想获取额外的response信息
	response_headers = response.info() --> 获取响应头的内容
	response_url = response.geturl()   --> 获取响应的url

使用代理IP爬取网页

# 通过自定义handler对象，然后封装成opener对象，并调用opener.open()方法发送请求，其他步骤相同

	# 使用公开代理IP
		http_proxyHandler = ur.ProxyHandler({'代理类型':'IP地址:端口号'})
		--> '代理类型' 是指 'http' 或者 'https'
		--> 'IP地址:端口号' 是指 'xxx.xx.xx.x:xx'

	# 使用私密代理IP（意味着需要账号和密码）
		http_proxyHandler = ur.ProxyHandler({'代理类型}:'账号:密码@IP地址:端口号')
		--> '代理类型' 是指 'http' 或者 'https'
		--> 'IP地址:端口号' 是指 'xxx.xx.xx.x:xx'
		--> '账号:密码' 一般会用系统变量代替，以防别人拿到源码后知道，因此
					  先进入~/.bash_profile中，设置export x='账号'
					                            export xx='密码'
					  并source ~/.bash_profile生效，
					  然后import os
					     x = os.getenv('x',default=xxx)   --> 取出'账号'的值，没找到值时返回xxx
					  	 xx = os.getenv('xx',default=xxx) --> 取出'密码'的值，没找到值时返回xxx

保存cookie来爬取网页

# 创建http.cookiejar.CookieJar()对象，并封装到HTTPCookieProcessor()处理器对象，
  然后封装成opener对象，调用open()方法发送请求

	  import http.cookiejar as hc
	  import urllib.request as ur

	# 使用”在线“cookie

	  cookie = hc.CookieJar()
	  cookie_handler = ur.HTTPCookieProcessor(cookie)
	  opener = ur.build_opener(cookie_handler)
	  opener.open(request对象)

	# 使用文件保存/读取cookie

	  file_cookie = hc.MozillaCookieJar()
	  # file_cookie.save(路径) --> 将cookie保存至本地文件
	  # file_cookie.load(路径) --> 从本地文件加载cookie
	  cookie_handler = ur.HTTPCookieProcessor(file_cookie)
	  opener = ur.build_opener(cookie_handler)
	  opener.open(request对象)
'''

'''
Requests库的使用
==========================================
import requests

1.1 构造并发送request请求

# 高级方法
  requests.request(...)                --> 返回response对象
			方法中的主要参数有：
			method                     --> 'get'/'post'/'head'/'put'/'delete'/'patch'
			url                        --> '有效的URL'
			params={...}               --> 查询字符串中的内容
			data={...}                 --> 请求体中form表单的内容
			headers={...}              --> 请求头中的内容
			cookies={...}/CookieJar    --> 手动填入Cookie值/CookieJar对象
			timeout=(conn,acce)        --> 请求超时设置，元组中conn是连接服务超时设置，acce是客户端接收超时设置
			proxies={...}              --> 代理IP设置，{'协议类型':'主机:端口'}
			verify=True/False          --> 是否验证服务器证书
			allow_redirects=True/False --> 是否允许请求重定向

# 低级方法(推荐使用)
  sess = requests.session()/.Session() --> 创建session对象
  										   (为了兼容，Session()和session()都能正确创建对象），
  										   session对象中重要的属性和方法：
	sess.headers
	sess.params
	sess.verify
	sess.cookies
	sess.proxies

	sess.close()                       --> 关闭会话

	sess.request(...)                  --> 返回response对象，上面的高级方法就是通过创建session对象并调用本方法实现的。
			方法中的主要参数有：
			method                     --> 'get'/'post'/'head'/'put'/'delete'/'patch'  /'options'
			url                        --> '有效的URL'
			params={...}               --> 查询字符串中的内容
			data={...}                 --> 请求体中form表单的内容
			headers={...}              --> 请求头中的内容
			cookies={...}/CookieJar    --> 手动填入Cookie值/CookieJar对象
			timeout=(conn,acce)        --> 请求超时设置，元组中conn是连接服务超时设置，acce是客户端接收超时设置
			proxies={...}              --> 代理IP设置，{'协议类型':'主机:端口'}
			verify=True/False          --> 是否验证服务器证书
			allow_redirects=True/False --> 是否允许请求重定向

# Cookies相关
	requests.cookies.RequestsCookieJar() --> 返回 CookieJar对象（兼容性较好）


1.2 处理response响应

# response.text          --> 返回 Unicode类型的响应内容
  response.content       --> 返回 bytes类型的响应内容
  response.json()        --> 返回 json格式的响应内容
  response.raw           --> 返回 原始的响应内容

  response.url           --> 返回 响应对应的最终url
  response.history       --> 返回 重定向产生的响应列表

  response.status_code   --> 返回 (响应的)状态码
  response.cookies       --> 返回 (响应的)cookies
  response.headers       --> 返回 (响应的)头部

'''

'''
Scrapy框架的使用
==========================================

a) 启动项目操作
-----------------------------------

在终端输入'scrapy startproject 项目名'来创建一个爬虫项目，该项目的目录结构如下：

   - 项目名							--> 爬虫项目的顶级目录

   |-- 项目名							--> 爬虫项目的根目录（所有的import和其他的cmd一般都在这个level键入）
         |-- __init__.py 			--> 模块包必要文件

         |-- items.py 				--> item文件（用于构建ORM，即对象关系映射）
         									--> 注意：1.import scrapy  2.新建的item类继承scrapy.Item类  3.在新建的item类中创建字段名（即scrapy.Field类的实例对象）
         |-- middlewares.py 		--> 中间件文件（用于存放各种中间件）

         |-- pipelines.py 			--> pipeline文件（用于处理从spider中yield的item数据）
											--> 注意：1.新建的管道类继承object  2.必须实现process_item(self,item,spider)方法  3.process_item方法中必须最后return item
         |-- settings.py 			--> 爬虫项目的设置文件（用于设置爬虫项目的各种信息）

         |-- spiders 				--> spider目录（用于存放所有的spider文件）
               |-- __init__.py 		--> 模块包必要文件

			   |-- 你要的spider.py 	--> spider文件（用于编写你的爬虫项目的主要业务逻辑）
			   								--> 注意：1.spider文件可以有很多个
   |-- scrapy.cfg 					--> 爬虫项目的配置文件（用于配置一些最基本的信息）


补充：1.scrapy库在__init__.py文件中设置了一些shortcuts，例如：
	     from scrapy.spiders import Spider
		 from scrapy.http import Request, FormRequest
		 from scrapy.selector import Selector
		 from scrapy.item import Item, Field

	   所以，import scrapy 						from scrapy.spiders import Spider

	   		class 类名(scrapy.Spider): 	等价于	class 类名(Spider):
	   			pass								pass


			import scrapy 						from scrapy.item import Item

			class 类名(scrapy.Item): 	等价于 	class 类名(Item):
				pass 								pass


b) item文件详解
-----------------------------------

from scrapy.item import Item, Field

class 类名(Item):
	xx = Field()
	.
	.
	.


c) spider文件详解
-----------------------------------

scrapy.spiders.Spider类是最基础的爬虫类，你还可以继承其他的爬虫类来实现更多的功能，例如：

--> from scrapy.spiders import CrawlSpider

  	class 类名(CrawlSpider):
  		pass

--> from scrapy.spiders import XMLFeedSpider

	class 类名(XMLFeedSpider):
		pass

--> from scrapy.spiders import CSVFeedSpider

	class 类名(CSVFeedSpider):
		pass

--> from scrapy.spiders import SitemapSpider

	class 类名(SitemapSpider):
		pass


	1.基于scrapy.spiders.Spider类的爬虫
	-----------------------------------

	from scrapy.spiders import Spider

	class 类名(Spider):

		name = 'xx' 					在终端运行'scrapy crawl xx'来启动你的爬虫程序
		allowed_domains = ['xx',...] 	字符串列表，用于限定爬虫爬取的域
		start_urls = ['xx',...] 		URL字符串列表，用于指定初始要爬取的URL地址
		custom_settings = {...} 		个性化设置，仅对当前的spider文件有效，优先级也高于settings文件中的设置
		handle_httpstatus_list = [...] 	当request请求对应的状态码位于列表中时，允许其返回对应的Response对象，
										因为默认只返回状态码为200-300的Response对象


		def parse(self,response): 		必须重写parse方法来处理返回的response对象
			...
			...
			xx = item文件中的类名()       解析函数中需要实例化item.py中定义的类，用于提取结构性数据
			...
			...

			yield xx 					如果是yield 数据，则会交给管道文件做进一步处理；
										如果是return 数据，则直接做处理，而不会交给管道文件


	补充：1.你可以重写def start_requests(self)方法来代替start_urls = [...]
		   start_requests() 方法本质上也是作为 '生成器函数' 来为爬虫提供 '初始要爬取的URL地址'

		 2.跟进发送request请求，详见下文


	2.基于scrapy.spiders.CrawlSpider类的爬虫：
	-----------------------------------

	CrawlSpider类提供了跟进response中的link，并跟进发送request的功能，更便于深度爬取

	from scrapy.spiders import CrawlSpider, Rule
	from scrapy.linkextractors import LinkExtractor

	class 类名(CrawlSpider):
		. 								同Spider类中的设置
		.
		.
		.
		rules = [Rule对象1,...] 			rules属性是容纳一个或多个Rule对象的列表

			Rule(LinkExtractor对象, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)
				--> LinkExtractor(...) ,唯一的作用就是指定按照什么规则跟进并提取URL,
										LinkExtractor对象拥有唯一的公用方法 extract_links(response) --> ctype: Link对象构成的列表
					--> LinkExtractor对象拥有以下参数：

						allow=(正则表达式或正则列表) 	--> 提取符合正则的URL，如果未指定正则，则默认提取所有URL
						deny=(正则表达式或正则列表) 	--> 提取不符合正则的URL，如果未指定正则，则默认不提取URL

						allow_domains=(...) 		--> 提取位于一个或多个域内的URL
						deny_domains=(...) 			--> 提取不位于一个或多个域内的URL

						unique=True/False 			--> 是否过滤重复的URL，默认为True

						process_value=函数 			--> 指定一个函数用于处理提取到的URL，返回处理后的URL或者None（即不做处理）

						strip=True/False 			--> 是否去掉空格

				--> callback='xx' ,指定跟进URL地址后拿到的response内容由哪个函数解析（不可以是'parse'）

						def xx(self,response,**kwargs):
							...
							...

				--> cb_kwargs={} ,指定传入callback函数中的参数（字典类型）

				--> follow=True/False ,指定是否对跟进的URL再跟进（即深度爬取）;如果callback=None,则follow默认为True，否则默认为False

				--> process_links='xx' ,指定LinkExtractor对象提取到的URL由哪个函数过滤

				--> process_request='xx' ,指定Rule对象提取到的URL发送request时由哪个函数进一步处理，该函数必须返回reqeust或者None（None一般说明该函数对request进行过滤）


		def xxx(self,response): 		xxx只要不是parse就可以
			...
			...
			提取结构性数据
			...
			...

			yield 数据 					如果是yield 数据，则会交给管道文件做进一步处理；
										如果是return 数据，则直接做处理，而不会交给管道文件


	补充：1.你可以重写def parse_start_url(self,response)方法来代替基础Spider类中的def parse(self,response)方法，用于解析初始的URL地址的response内容

		 2.跟进发送request请求，详见下文

		 3.发送request请求和response响应详解

		 	< Request对象 >

			import scrapy 						from scrapy.http import Request
									或者
			scrapy.Request(...) 				Request(...)
				--> ctype: Request对象

				--> url='xx'				--> 请求的URL地址（字符串形式）
					callback=func           --> 拿到请求的response指定对应的解析函数
					method='GET'/...        --> 请求的方式（字符串形式）
					body='xx'				--> 请求体内容（字符串形式）
					headers={...} 			--> 请求头内容（字典形式）
					cookies={...} 			--> 请求的cookies内容（字典形式）
					priority=数字  			--> 请求发送的优先级（数字越大优先级越高，请求越先发送，默认为0）
					dont_filter=False/True 	--> 默认为False，即对重复的URL请求进行过滤；如果需要对同一URL发送多次请求，应该设为True

				--> .url ,即只读属性，返回请求中的URL（字符串形式）
				--> .method ,即返回请求中的HTTP方法（字符串形式，全大写）
				--> .headers ,即返回请求头的内容（类字典形式）
				--> .body ,即只读属性，请求体的内容（字符串形式）
				--> .replace([url,method,headers,body,cookies,meta,encoding,dont_filter,callback,errback])
						--> ctype: Request对象，即通过'关键字参数'修改请求中的内容

				--> .meta = {...} ,即当前request请求独有的元数据，在新建的request请求中.meta为空字典，
								   .meta一般用于middlewares或extensions识别使用，.meta中的key值可以自定义，也可以有：

						--> 为当前的request请求设置代理信息
						proxy 						.meta['proxy'] = 'http://IP地址:端口号'或者'http://用户名:密码@IP地址:端口号'


						--> 根据Response对象的状态码设置重定向操作，在settings.py中可以设置'REDIRECT_ENABLED'和'REDIRECT_MAX_TIMES'
						redirect_urls 				.meta['redirect_urls'] = 'URL' ,即设置当前的request请求重定向时的URL
						dont_redirect 				.meta['dont_redirect'] = True/False ,即如果为True则不允许当前的request请求进行重定向操作
						handle_httpstatus_list 		.meta['handle_httpstatus_list] = [状态码,...] ,即如果当前的request请求对应的状态码在列表中，
														则允许程序返回对应的Response对象供我们处理（因为scrapy默认只处理状态码在200-300之间的Response对象）
						handle_httpstatus_all 		.meta['handle_httpstatus_all] = True/False ,即同上，如果为True则允许程序返回任何状态码的Response对象供我们处理


						--> 当前的request请求如果失败时，是否重新发送的设置，在settings.py中可以设置'RETRY_ENABLED', 'RETRY_TIMES'和'RETRY_HTTP_CODES'
						dont_retry 					.meta['dont_retry'] = True/False ,即如果为True，则当前的request请求如果失败时不重新发送
						max_retry_times 			.meta['max_retry_times;] = 次数 ,即设置最多重新发送多少次


						--> 当前的request请求的cookies设置，在setting.py中可以设置'COOKIES_ENABLED'
						cookiejar 					.meta['cookiejar'] = 身份数字 ,即一个spider中可以设置多个Cookie（默认只有一个），
														当前request请求发送时可以指定{'cookiejar': 身份数字}从而使用其他的Cookie，但无法根据'身份数字'便捷地复用该Cookie，在Response中复用需要Response.meta['cookiejar']取值
						dont_merge_cookies 			.meta['dont_merge_cookies'] = True/False ,即如果为True，则当前的request请求对应的response中的Cookie不会加入已存在的Cookie中


						--> 设置当前的request请求是否遵循Robots协议，它的优先级高于setting.py文件中的设置
						dont_obey_robotstxt 		.meta['dont_obey_robotstxt'] = True/False ,即如果为True，则当前的request请求不遵循Robots协议


						--> 下载器的相关设置
						download_timeout 			.meta['download_timeout'] = 秒数 ,即下载器下载超时时最多等待多少秒
						download_maxsize 			.meta['download_maxsize'] = 字节数 ,即下载器下载到的Response对象最大允许为多少字节


						bindaddress


						ftp_user
						ftp_password


						referrer_policy


						dont_cache


				--> FormRequest(...)
						--> ctype: FormRequest对象，即Request()的子类，(...)在上述参数的基础上添加了formdata={...}参数，
																	用于向body(即请求体)中添加'表单信息'
						--> FormRequest.from_response(...)
								--> ctype: 新的FormRequest对象，即FormRequest的类方法from_response()
								--> response ,即response对象

								--> formname='xx' ,
								--> formid='xx' ,

								--> formxpath='xx' ,
								--> formcss='xx' ,
								--> formumber=整数 ,

								--> formdata={...} ,

								--> clickdata={...} ,
								--> dont_click=True/False ,

			< Response对象 >

			from scrapy.http import Response(...)

				--> ctype: Response对象

				--> url='xx' 				--> 响应的URL（字符串形式）
				--> status=数字 				--> 响应的状态码（默认为200）
				--> headers={...} 			--> 响应头的内容（字典形式）
				--> body=b'xx' 				--> 响应体的内容（字节形式）
				--> request=None 			--> 响应对应的Request对象（对象形式）

				--> .url ,即只读属性，当前的Response对象中的URL（字符串形式）
				--> .status ,即当前的Response对象的状态码（整数形式）
				--> .headers ,即当前的Response对象的响应头信息（类字典形式）
				--> .body ,即只读属性，当前的Response对象的响应体信息（一定是bytes类型）
				--> .request ,即生成当前的Response对象对应的Request对象
				--> .meta ,即等价于 Request对象.meta 属性
				--> .urljoin('相对路径') ,即返回绝对路径（字符串形式），将当前的Response对象的URL和参数中的'相对路径'拼接在一起
				--> .replace([url,status,headers,body,request,flags,cls])
						--> ctype: Response对象，即通过'关键字参数'修改响应中的内容

				--> TextResponse(...)
						--> ctype: TextResponse对象，即Response类的子类，(...)在上述参数的基础上添加了encoding='xx'参数，
																	  用于表示当前的Response对象的编码格式是什么
						--> .text ,即返回Unicode字符串格式的响应体内容（推荐使用）
						--> .body_as_unicode() ,即返回Unicode字符串格式的响应体内容（等价于.text,被用于向后兼容）

						--> .selector ,即返回一个以当前Response对象为目标的Selector实例对象
						--> .xpath(...) ,即等价于.selector.xpath(...)，相当于一个shortcut
						--> .css(...) ,即等价于.selector.css(...)，相当于一个shortcut


e) pipelines文件详解
-----------------------------------

管道文件通常用于：清理HTML数据，验证item数据是否包含需要的内容，将数据储存到数据库中等等

--> class 类名(object):
		def process_item(self,item,spider):
			xxx
			return item

补充：1.管道类中必须重写def process_item(self,item,spider)方法，
	 2.上述方法必须return dict或item，或者raise DropItem异常
	 3.管道类中还可以重写以下方法：

	 	def open_spider(self,spider):		--> 爬虫程序开启时调用该方法
	 		pass

	 	def close_spider(self,spider): 		--> 爬虫程序关闭时调用该方法
	 		pass

	 4.下载files或images时，可以借助scrapy自带的FilesPipeline或ImagesPipeline，
	   本质上，在spider文件中yield files或images的URL，程序会自动发送请求并处理队列，将response内容映射到内置字段中，

	   在常规操作的基础上增加了：

	   4.0) 安装Pillow库

	   4.1) items.py文件中新增字段 --> file_urls = Field()  或  image_urls = Field()
	   								files = Field()      者  images = Field()

	   						    --> 自定义的file_urls = Field()  或  自定义的image_urls = Field()
	   								自定义的files = Field()      者  自定义的images = Field()

	   4.2) settings.py文件中新增设置 --> ITEM_PIPELINES = {'scrapy.pipelines.files.FilesPipeline': 权重}  或者
									   ITEM_PIPELINES = {'scrapy.pipelines.images.ImagesPipeline': 权重}

								   --> FILES_STORE = '文件储存路径'  或者
								   	   IMAGES_STORE = '图片储存路径'    --> 最终该路径下会生成'full'子目录用于储存爬取到的图片，'thumbs'用于储存缩略图（如果配置了的话）

										   --> IMAGES_THUMBS = {'缩略图名': (宽, 高), ...}  --> 设置缩略图的信息，'缩略图名'就是储存该类缩略图的目录名，
										   													  例如：IMAGES_THUMBS = {'small': (20,20), 'big': (120, 120)}
										   --> IMAGES_MIN_WIDTH = 数字  --> 设置下载图片的最小宽度（用于图片过滤，可以仅设置最小宽度，也可以都设置；都设置时，全都满足的图片才会被下载）

										   --> IMAGES_MIN_HEIGHT = 数字 --> 设置下载图片的最小高度（用于图片过滤，可以仅设置最小高度，也可以都设置；都设置时，全都满足的图片才会被下载）

								   --> FILES_URLS_FIELD = '自定义的file_urls'  或  IMAGES_URLS_FIELD = '自定义的image_urls'
								   	   FILES_RESULT_FIELD = '自定义的files'    者  IMAGES_RESULT_FIELD = '自定义的images'

								   --> FILES_EXPIRES = 90(单位：天)  或者  --> 设置天数（默认90天），避免下载近期下载过的内容
								   	   IMAGES_EXPIRES = 90(单位：天)

								   --> MEDIA_ALLOW_REDIRECTS = True/False  --> 设置下载file或image时，是否允许重定向

								   --> 如果你通过继承和重写设置了多个pipeline，那么你可以在settings.py文件中在'默认设置项'前加上'全大写的'自定义pipeline类名，
								   	   从而让你自定义的pipeline生效，例如：IMAGES_URLS_FIELD 应该为 MYPIPELINE_IMAGES_URLS_FIELD

	   4.3) spider.py文件中提取file_url或image_url，创建item对象并yield给管道处理：item['字段名'] = [file_url或image_url]  切记！！！以列表形式yield

	   4.4) 定制化处理，自定义file或image管道，其实只要：
				1.继承FilesPipeline类（from scrapy.pipelines.files import FilesPipeline）或
				     ImagesPipeline类（from scrapy.pipelines.images import ImagesPipeline），
				2.重写 get_media_requests(self, item ,info) 和/或 item_completed(self, results, item, info) 方法
				3.重写 file_path(self, request, response=None, info=None) 和 thumb_path(self, request, thumb_id, response=None, info=None) 方法

				get_media_requests(item,info)
					--> 该方法的本质是遍历item[key]，对每个URL重新yield Request(...)，
						注意：key对应的value是列表形式，因为spider.py中yield的是列表；至于key是什么，取决于items.py中的字段名

					--> 例如：def get_media_requests(self, item, info):
							    for file_url in item['file_urls']:
							        yield scrapy.Request(file_url)

				item_completed(results,item,info):
					--> 当【1个item】中所有的URL请求都发送完毕后（无论结果是OK，还是Failed），就会调用该方法，
						其本质是过滤出results中状态为OK（即下载成功）的数据并return，以便其他的pipeline处理它，

						注意:results参数是一个二元元组构成的列表，形如：(success,file_info_or_error)，
							其中，file_info_or_error是一个字典，形如：{'url': '数据下载的来源', 'path': '相对于settings中xx_STORE的相对路径', 'checksum': '数据的MD5哈希值'}

						示例:[(True, {'checksum': '2b00042f7481c7b056c4b410d28f33cf',
   									 'path': 'full/0a79c461a4062ac383dc4fade7bc09f1384a3910.jpg',
   									 'url': 'http://www.example.com/files/product1.pdf'}),
 							 (False, Failure(...)),
 							 ...
 							]

 					--> 例如：from scrapy.exceptions import DropItem

							def item_completed(self, results, item, info):
							    file_paths = [x['path'] for ok, x in results if ok]
							    if not file_paths:
							        raise DropItem("Item contains no files")
							    item['file_paths'] = file_paths   ## 如果是图片，则改为item['image_paths] = xxx；如果是自定义的字段，则改为item['自定义字段名'] = xxx；
							    return item

				file_path(self, request, response=None, info=None):
					...
					return 'full/%s' % (自定义文件名,)

					--> 例如：def file_path(self, request, response=None, info=None):
						        image_path = request.url.split('/')[-1]
						        return 'full/%s' % (image_path,)

				thumb_path(self, request, thumb_id, response=None, info=None):
					...
					return 'thumbs/%s/%s' % (thumb_id, 自定义缩略图名)

					--> 例如：def thumb_path(self, request, thumb_id, response=None, info=None):
						        thumb_path = request.url.split('/')[-1]
						        return 'thumbs/%s/%s' % (thumb_id, thumb_path,)


f) middlewares文件详解
-----------------------------------

1.下载中间件是处理request和response的框架，可以如下设置（如果权重设为None，则表示禁用该中间件）：

	DOWNLOADER_MIDDLEWARES = {
    	'myproject.middlewares.CustomDownloaderMiddleware': 543,
    	'xxxx': None,
	}

2.下载中间件是 DOWNLOADER_MIDDLEWARES 和 DOWNLOADER_MIDDLEWARES_BASE 的合集，

3.下载中间件可以设置多个，如果存在多个：
  在处理request时，会按照权重升序（由小到大）依次调用每个中间件的process_request()方法
  在处理response时，会按照权重降序（由大到小）依次调用每个中间件的process_response()方法

4.编写自己的下载中间件 -> 定义一个实现了process_request(), process_response(), process_exception()等一个或多个方法的Python类

	def process_request(self,request,spider):
		--> ctype: None/Request对象/Response对象/raise IgnoreRequest异常
		--> 如果返回None，则表示该中间件不做任何加工处理，直接将request对象交给下一个中间件处理；
			如果返回Request对象，则程序不再调用（后续中间件中的）process_request()方法，而是直接将这个返回的Request对象重新入队列
							（之后拿到的response对象仍然会正常走一系列中间件的process_response()方法调用）；
			如果返回Response对象，则程序不再调用任何其他的process_request()和process_exception()方法，
							   而是直接返回这个Response对象（开启中间件中process_response()方法的依次调用）
			如果抛出IgnoreRequest异常，则调用process_exception()方法，如果没有则依次向上传递，直到顶层时被忽略（不记入log中，这点与一般的异常处理不同）

	def process_response(self,request,response,spider):
		--> ctype: Request对象/Response对象/raise IgnoreRequest异常
		--> 如果返回Request对象，则程序不再调用（后续中间件中的）process_response()方法，而是直接将这个返回的Request对象重新入队列；
		--> 如果返回Response对象，则程序将这个返回的Response对象（可以不变，也可以是新的）交给下一个中间件处理；
			如果抛出IgnoreRequest异常，则依次向上传递异常，直到顶层时被忽略（不记入log中，这点与一般的异常处理不同）

	def process_exception(self,request,exception,spider):
		--> ctype: None/Request对象/Response对象
		--> 如果返回None，则程序将依次调用下一个中间件的process_exception()方法，直到异常被处理或者被抛到顶层时忽略；
			如果返回Request对象，则程序不再调用（后续中间件中的）process_exception()方法，而是直接将这个返回的Request对象重新入队列；
			如果返回Response对象，则程序不再调用（后续中间件中的）process_exception()方法，
							   而是直接返回这个Response对象（开启中间件中process_response()方法的依次调用）

5.编写下载中间件的本质就是对Request对象和Response对象的属性做修改，详见上文


g) settings文件详解
-----------------------------------

CONCURRENT_ITEMS
	--> 默认100
	--> 每个Response可以并发处理的最大item数量

CONCURRENT_REQUESTS
	--> 默认16
	--> 可以并发请求的最大数量

CONCURRENT_REQUESTS_PER_DOMAIN
	--> 默认8
	--> 单个域中可以并发请求的最大数量

DEFAULT_REQUEST_HEADERS
	--> 默认的请求头设置

DOWNLOAD_DELAY
	--> 默认0
	--> 设置连续下载页面之间的间隔时间（即延时时间）

DUPEFILTER_CLASS
	--> 默认'scrapy.dupefilters.RFPDupeFilter'
	--> 设置默认的去重类





h) Redis分布式爬虫
-----------------------------------

1.安装scrapy-redis库:
	pip3 install scrapy-redis

2.开启Master端的redis-server，并注释掉"bind 127.0.0.1"配置项或者添加多个"bind 爬虫端的IP地址":
	爬虫端无需启动redis-server，只要能够连接Master端的redis-server进行数据读/写即可

3.在原有scrapy项目的基础上调整部分代码：
	a) spider文件
    	from scrapy_redis.spiders import RedisSpider, RedisCrawlSpider
    	-> 原有scrapy项目中，继承spider类的现在继承RedisSpider类，
    	                   继承CrawlSpider类的现在继承RedisCrawlSpider类，

    	删除"start_urls = [...]"代码，使用：
    	-> redis_key = 'xxx'
    	   'xxx'建议为'myspider:start_urls',这个字符串被用作Master端redis-server推送初始URL的命令符之一；

	b) settings文件

		- 指定使用scrapy-redis的调度器
		SCHEDULER = "scrapy_redis.scheduler.Scheduler"

		- 指定使用scrapy-redis的去重
		DUPEFILTER_CLASS = 'scrapy_redis.dupefilters.RFPDupeFilter'

		- 指定排序爬取地址时使用的队列，
		- 默认的 按优先级排序(Scrapy默认)，由sorted set实现的一种非FIFO、LIFO方式。
		SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.SpiderPriorityQueue'

			- 可选的 按先进先出排序（FIFO）
			SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.SpiderQueue'

			- 可选的 按后进先出排序（LIFO）
			SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.SpiderStack'


			- 只在使用SpiderQueue或者SpiderStack是有效的参数，指定爬虫关闭的最大间隔时间
			SCHEDULER_IDLE_BEFORE_CLOSE = 10

		- 在redis中保持scrapy-redis用到的各个队列，从而允许暂停和暂停后恢复，也就是不清理redis queues
		SCHEDULER_PERSIST = True

		- 通过配置RedisPipeline将item写入key为 spider.name : items 的redis的list中，供后面的分布式处理item
		ITEM_PIPELINES = {
		    'example.pipelines.ExamplePipeline': 300,
		    'scrapy_redis.pipelines.RedisPipeline': 400
		}

		- 指定redis数据库的连接参数
		REDIS_HOST = '127.0.0.1'

		REDIS_PORT = 6379

		REDIS_PASS = 'redisP@ssw0rd'

4.将修改后的scrapy项目代码分发到各个爬虫端，并在爬虫文件同级目录的终端下输入：
	scrapy runspider 爬虫文件.py

5.在Master端的redis-cli交互模式下输入：
	lpush xxx 初始URL
	-> xxx 即 redis_key的值（不带引号）

'''





